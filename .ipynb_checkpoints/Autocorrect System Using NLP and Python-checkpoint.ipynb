{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b836e2-72ae-41e5-83cc-4aa20bb9e219",
   "metadata": {},
   "source": [
    "Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f0616c-e4ef-459a-a95d-8ca19e8b76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textdistance\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af780c63-eff3-4b6a-a45e-8aac7b313f92",
   "metadata": {},
   "source": [
    "Step 2: Load and Preprocess the Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ec459-0bbb-4893-8be5-23d4632fb86b",
   "metadata": {},
   "source": [
    "We use a text file as our corpus. Here, the text is converted to lowercase, and words are extracted using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3658f1c5-ba42-4881-9452-4b374434694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the corpus: 17647\n"
     ]
    }
   ],
   "source": [
    "# Load the text data with UTF-8 encoding\n",
    "with open('book.txt', 'r', encoding='utf-8') as f:\n",
    "    text_data = f.read().lower()\n",
    "\n",
    "# Extract words using regular expressions\n",
    "words = re.findall(r'\\w+', text_data)\n",
    "\n",
    "# Create a set of unique words (vocabulary)\n",
    "vocabulary = set(words)\n",
    "print(f\"Total unique words in the corpus: {len(vocabulary)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc37746-c127-4e66-a1d8-fbccfbcc49b8",
   "metadata": {},
   "source": [
    "Step 3: Build Word Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126ab24-0d59-495d-9edc-cf2a3bf39eb4",
   "metadata": {},
   "source": [
    "To calculate the likelihood of words, we first need to build a frequency distribution of words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d24d0dc6-c480-4ac7-935a-a0bd438b2446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words: [('the', 14703), ('of', 6742), ('and', 6517), ('a', 4799), ('to', 4707), ('in', 4238), ('that', 3081), ('it', 2534), ('his', 2530), ('i', 2120)]\n"
     ]
    }
   ],
   "source": [
    "# Calculate word frequency using Counter\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Display the most common words\n",
    "print(\"Most common words:\", word_freq.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1de57c-670b-4e77-9f6e-5770563aa2cf",
   "metadata": {},
   "source": [
    "Step 4: Calculate Word Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407988cc-5d9d-46a2-b267-0ec839306a03",
   "metadata": {},
   "source": [
    "Next, we compute the relative probabilities of each word based on its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8cb5e7e-d976-4d7f-8ecf-c25705fd99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of words\n",
    "total_words = sum(word_freq.values())\n",
    "\n",
    "# Calculate probability of each word\n",
    "word_probs = {word: freq / total_words for word, freq in word_freq.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c06133-d63b-4811-ac4f-84804f821961",
   "metadata": {},
   "source": [
    "Step 5: Define the Autocorrect Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe4d29-ce89-4379-98d0-7b7f5c544f84",
   "metadata": {},
   "source": [
    "This function takes an input word and attempts to find the closest match from the vocabulary using the Jaccard distance, which measures similarity between two sets of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b947168-01c6-4c53-83f0-896a63052a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect(input_word):\n",
    "    \"\"\"\n",
    "    Function to autocorrect the input word based on the closest match from the vocabulary.\n",
    "    \n",
    "    Parameters:\n",
    "    input_word (str): The word to be corrected.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the most similar words and their probabilities.\n",
    "    \"\"\"\n",
    "    input_word = input_word.lower()\n",
    "    \n",
    "    # Check if the word is already correct\n",
    "    if input_word in vocabulary:\n",
    "        return f\"'{input_word}' is already correct.\"\n",
    "    \n",
    "    # Calculate similarity between input word and words in the vocabulary\n",
    "    similarities = [1 - textdistance.Jaccard(qval=2).distance(word, input_word) for word in word_freq.keys()]\n",
    "    \n",
    "    # Create a DataFrame with words, probabilities, and similarities\n",
    "    df = pd.DataFrame({'Word': list(word_freq.keys()), \n",
    "                       'Probability': list(word_probs.values()), \n",
    "                       'Similarity': similarities})\n",
    "    \n",
    "    # Sort the DataFrame by similarity and probability\n",
    "    df = df.sort_values(by=['Similarity', 'Probability'], ascending=False)\n",
    "    \n",
    "    # Return the top 5 suggestions\n",
    "    return df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c55eb2c-5ef2-490f-a7cd-f1a0cf1d40a0",
   "metadata": {},
   "source": [
    "Step 6: Test the Autocorrect Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce438087-1cb1-405e-b5d3-113be3ffb07c",
   "metadata": {},
   "source": [
    "Finally, let's test the autocorrect function with some sample input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0028c730-5b91-4a85-9406-c9a8efae0c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Word  Probability  Similarity\n",
      "2571   nevertheless     0.000225    0.750000\n",
      "13657      boneless     0.000013    0.416667\n",
      "12684      elevates     0.000004    0.416667\n",
      "1105          never     0.000925    0.400000\n",
      "7136          level     0.000108    0.400000\n"
     ]
    }
   ],
   "source": [
    "# Test the autocorrect function\n",
    "suggestions = autocorrect('neverteless')\n",
    "print(suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4cc1e-5026-45d2-ba4f-575f24ba4193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6e007-1a67-4179-b7ad-b9d19fe23316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "288cc5a2-ca4f-4d61-ad34-70590ea1a0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top corrections for 'neverteless':\n",
      "           Word  Similarity  Probability\n",
      "0  nevertheless        0.75     0.000225\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import textdistance\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load text data and create vocabulary\n",
    "def load_text_data(file_path):\n",
    "    \"\"\"\n",
    "    Load text data from a file, clean it, and create a set of unique words (vocabulary).\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): The path to the text file.\n",
    "    \n",
    "    Returns:\n",
    "    - set: A set containing all unique words in the text.\n",
    "    - Counter: A Counter object containing word frequencies.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Read and clean the text data\n",
    "        text_data = file.read().lower()\n",
    "        \n",
    "    # Extract words using regular expressions\n",
    "    words = re.findall(r'\\w+', text_data)\n",
    "    \n",
    "    # Create a set of unique words (vocabulary) and word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    vocabulary = set(word_freq.keys())\n",
    "    \n",
    "    return vocabulary, word_freq\n",
    "\n",
    "# Function to calculate word probabilities\n",
    "def calculate_probabilities(word_freq):\n",
    "    \"\"\"\n",
    "    Calculate the probabilities of words based on their frequencies.\n",
    "    \n",
    "    Parameters:\n",
    "    - word_freq (Counter): A Counter object containing word frequencies.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing word probabilities.\n",
    "    \"\"\"\n",
    "    total_words = sum(word_freq.values())\n",
    "    return {word: freq / total_words for word, freq in word_freq.items()}\n",
    "\n",
    "# Function to generate similar words using edit distance\n",
    "def generate_candidates(word, max_edits=2):\n",
    "    \"\"\"\n",
    "    Generate candidate words that are within a certain edit distance of the input word.\n",
    "    \n",
    "    Parameters:\n",
    "    - word (str): The input word to generate candidates for.\n",
    "    - max_edits (int): The maximum number of edits allowed. Default is 2.\n",
    "    \n",
    "    Returns:\n",
    "    - set: A set of candidate words.\n",
    "    \"\"\"\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    # Splits, deletions, transpositions, and replacements\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    \n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# Function to get the best correction for a misspelled word\n",
    "def autocorrect(word, vocabulary, word_freq, word_probs):\n",
    "    \"\"\"\n",
    "    Get the most likely correction for a misspelled word based on edit distance and word probability.\n",
    "    \n",
    "    Parameters:\n",
    "    - word (str): The input word to autocorrect.\n",
    "    - vocabulary (set): The set of unique words in the corpus.\n",
    "    - word_freq (Counter): A Counter object containing word frequencies.\n",
    "    - word_probs (dict): A dictionary containing word probabilities.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the most similar words with their probabilities.\n",
    "    \"\"\"\n",
    "    # If the word is in the vocabulary, it's already correct\n",
    "    if word in vocabulary:\n",
    "        return f\"'{word}' is spelled correctly.\"\n",
    "    \n",
    "    # Generate candidate words within a certain edit distance\n",
    "    candidates = (generate_candidates(word) & vocabulary) or [word]\n",
    "    \n",
    "    # Calculate similarities and probabilities for candidates\n",
    "    similarities = [(candidate, 1 - textdistance.Jaccard(qval=2).distance(word, candidate))\n",
    "                    for candidate in candidates]\n",
    "    \n",
    "    # Create a DataFrame with candidates, similarities, and probabilities\n",
    "    df = pd.DataFrame(similarities, columns=['Word', 'Similarity'])\n",
    "    df['Probability'] = df['Word'].apply(lambda w: word_probs.get(w, 0))\n",
    "    \n",
    "    # Sort by similarity and probability\n",
    "    df = df.sort_values(by=['Similarity', 'Probability'], ascending=False).head(5)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main function to demonstrate the autocorrect functionality\n",
    "if __name__ == \"__main__\":\n",
    "    # Load text data and create vocabulary\n",
    "    vocabulary, word_freq = load_text_data('book.txt')\n",
    "    \n",
    "    # Calculate word probabilities\n",
    "    word_probs = calculate_probabilities(word_freq)\n",
    "    \n",
    "    # Test the autocorrect function\n",
    "    word_to_correct = 'neverteless'\n",
    "    corrections = autocorrect(word_to_correct, vocabulary, word_freq, word_probs)\n",
    "    print(f\"Top corrections for '{word_to_correct}':\\n{corrections}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6cb675-db00-4f0c-94f8-697b20def96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85340575-da26-4f2b-96c0-dcaf2de328d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "autocorrect() missing 1 required positional argument: 'word_probs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 115\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Test the autocorrect function\u001b[39;00m\n\u001b[0;32m    114\u001b[0m word_to_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneverteless\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 115\u001b[0m corrections \u001b[38;5;241m=\u001b[39m \u001b[43mautocorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_to_correct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop corrections for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_to_correct\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcorrections\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: autocorrect() missing 1 required positional argument: 'word_probs'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import textdistance\n",
    "\n",
    "# Function to load text data and create vocabulary\n",
    "def load_text_data(file_path):\n",
    "    \"\"\"\n",
    "    Load text data from a file, clean it, and create a set of unique words (vocabulary).\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): The path to the text file.\n",
    "    \n",
    "    Returns:\n",
    "    - set: A set containing all unique words in the text.\n",
    "    - Counter: A Counter object containing word frequencies.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Read and clean the text data\n",
    "        text_data = file.read().lower()\n",
    "        \n",
    "    # Extract words using regular expressions\n",
    "    words = re.findall(r'\\w+', text_data)\n",
    "    \n",
    "    # Create a set of unique words (vocabulary) and word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    vocabulary = set(word_freq.keys())\n",
    "    \n",
    "    # Release memory used by the raw text\n",
    "    del text_data, words\n",
    "    \n",
    "    return vocabulary, word_freq\n",
    "\n",
    "# Function to calculate word probabilities\n",
    "def calculate_probabilities(word_freq):\n",
    "    \"\"\"\n",
    "    Calculate the probabilities of words based on their frequencies.\n",
    "    \n",
    "    Parameters:\n",
    "    - word_freq (Counter): A Counter object containing word frequencies.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing word probabilities.\n",
    "    \"\"\"\n",
    "    total_words = sum(word_freq.values())\n",
    "    return {word: freq / total_words for word, freq in word_freq.items()}\n",
    "\n",
    "# Function to generate similar words using edit distance\n",
    "def generate_candidates(word, max_edits=2):\n",
    "    \"\"\"\n",
    "    Generate candidate words that are within a certain edit distance of the input word.\n",
    "    \n",
    "    Parameters:\n",
    "    - word (str): The input word to generate candidates for.\n",
    "    - max_edits (int): The maximum number of edits allowed. Default is 2.\n",
    "    \n",
    "    Returns:\n",
    "    - set: A set of candidate words.\n",
    "    \"\"\"\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    # Splits, deletions, transpositions, and replacements\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    \n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# Function to get the best correction for a misspelled word\n",
    "def autocorrect(word, vocabulary, word_freq, word_probs):\n",
    "    \"\"\"\n",
    "    Get the most likely correction for a misspelled word based on edit distance and word probability.\n",
    "    \n",
    "    Parameters:\n",
    "    - word (str): The input word to autocorrect.\n",
    "    - vocabulary (set): The set of unique words in the corpus.\n",
    "    - word_freq (Counter): A Counter object containing word frequencies.\n",
    "    - word_probs (dict): A dictionary containing word probabilities.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of tuples containing the most similar words with their probabilities.\n",
    "    \"\"\"\n",
    "    # If the word is in the vocabulary, it's already correct\n",
    "    if word in vocabulary:\n",
    "        return [(word, word_probs[word], 1.0)]\n",
    "    \n",
    "    # Generate candidate words within a certain edit distance\n",
    "    candidates = (generate_candidates(word) & vocabulary) or [word]\n",
    "    \n",
    "    # Calculate similarities and probabilities for candidates\n",
    "    results = [\n",
    "        (candidate, word_probs.get(candidate, 0), 1 - textdistance.Jaccard(qval=2).distance(word, candidate))\n",
    "        for candidate in candidates\n",
    "    ]\n",
    "    \n",
    "    # Sort by similarity and probability, and return top results\n",
    "    results.sort(key=lambda x: (-x[2], -x[1]))\n",
    "    \n",
    "    return results[:5]\n",
    "\n",
    "# Main function to demonstrate the autocorrect functionality\n",
    "if __name__ == \"__main__\":\n",
    "    # Load text data and create vocabulary\n",
    "    vocabulary, word_freq = load_text_data('book.txt')\n",
    "    \n",
    "    # Calculate word probabilities\n",
    "    word_probs = calculate_probabilities(word_freq)\n",
    "    \n",
    "    # Free up memory used by word_freq after calculating probabilities\n",
    "    del word_freq\n",
    "    \n",
    "    # Test the autocorrect function\n",
    "    word_to_correct = 'neverteless'\n",
    "    corrections = autocorrect(word_to_correct, vocabulary, word_probs)\n",
    "    print(f\"Top corrections for '{word_to_correct}':\\n{corrections}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df7a9dc-ce11-430a-ad8f-4df2769575df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
